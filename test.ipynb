{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-25T19:32:22.355824Z",
     "start_time": "2024-09-25T19:32:22.000492Z"
    }
   },
   "source": [
    "import re\n",
    "import feedparser\n",
    "import requests\n",
    "\n",
    "query = \"ti:Prompt Cache modular attention\"\n",
    "\n",
    "url = f\"https://export.arxiv.org/api/query?search_query={requests.utils.quote(query)}&max_results=5\"\n",
    "response = requests.get(url)\n",
    "if response.status_code != 200:\n",
    "    print(\"Error accessing arXiv API.\")\n",
    "# \n",
    "# {'id': 'http://arxiv.org/abs/2311.04934v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2311.04934v2',\n",
    "#  'updated': '2024-04-25T15:45:19Z',\n",
    "#  'updated_parsed': time.struct_time(tm_year=2024, tm_mon=4, tm_mday=25, tm_hour=15, tm_min=45, tm_sec=19, tm_wday=3,\n",
    "#                                     tm_yday=116, tm_isdst=0), 'published': '2023-11-07T18:17:05Z',\n",
    "#  'published_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=7, tm_hour=18, tm_min=17, tm_sec=5, tm_wday=1,\n",
    "#                                       tm_yday=311, tm_isdst=0),\n",
    "#  'title': 'Prompt Cache: Modular Attention Reuse for Low-Latency Inference',\n",
    "#  'title_detail': {'type': 'text/plain', 'language': None, 'base': '',\n",
    "#                   'value': 'Prompt Cache: Modular Attention Reuse for Low-Latency Inference'},\n",
    "#  'summary': 'We present Prompt Cache, an approach for accelerating inference for large\\nlanguage models (LLM) by reusing attention states across different LLM prompts.\\nMany input prompts have overlapping text segments, such as system messages,\\nprompt templates, and documents provided for context. Our key insight is that\\nby precomputing and storing the attention states of these frequently occurring\\ntext segments on the inference server, we can efficiently reuse them when these\\nsegments appear in user prompts. Prompt Cache employs a schema to explicitly\\ndefine such reusable text segments, called prompt modules. The schema ensures\\npositional accuracy during attention state reuse and provides users with an\\ninterface to access cached states in their prompt. Using a prototype\\nimplementation, we evaluate Prompt Cache across several LLMs. We show that\\nPrompt Cache significantly reduce latency in time-to-first-token, especially\\nfor longer prompts such as document-based question answering and\\nrecommendations. The improvements range from 8x for GPU-based inference to 60x\\nfor CPU-based inference, all while maintaining output accuracy and without the\\nneed for model parameter modifications.',\n",
    "#  'summary_detail': {'type': 'text/plain', 'language': None, 'base': '',\n",
    "#                     'value': 'We present Prompt Cache, an approach for accelerating inference for large\\nlanguage models (LLM) by reusing attention states across different LLM prompts.\\nMany input prompts have overlapping text segments, such as system messages,\\nprompt templates, and documents provided for context. Our key insight is that\\nby precomputing and storing the attention states of these frequently occurring\\ntext segments on the inference server, we can efficiently reuse them when these\\nsegments appear in user prompts. Prompt Cache employs a schema to explicitly\\ndefine such reusable text segments, called prompt modules. The schema ensures\\npositional accuracy during attention state reuse and provides users with an\\ninterface to access cached states in their prompt. Using a prototype\\nimplementation, we evaluate Prompt Cache across several LLMs. We show that\\nPrompt Cache significantly reduce latency in time-to-first-token, especially\\nfor longer prompts such as document-based question answering and\\nrecommendations. The improvements range from 8x for GPU-based inference to 60x\\nfor CPU-based inference, all while maintaining output accuracy and without the\\nneed for model parameter modifications.'},\n",
    "#  'authors': [{'name': 'In Gim'}, {'name': 'Guojun Chen'}, {'name': 'Seung-seob Lee'}, {'name': 'Nikhil Sarda'},\n",
    "#              {'name': 'Anurag Khandelwal'}, {'name': 'Lin Zhong'}], 'author_detail': {'name': 'Lin Zhong'},\n",
    "#  'author': 'Lin Zhong', 'arxiv_comment': 'To appear at MLSys 2024',\n",
    "#  'links': [{'href': 'http://arxiv.org/abs/2311.04934v2', 'rel': 'alternate', 'type': 'text/html'},\n",
    "#            {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.04934v2', 'rel': 'related', 'type': 'application/pdf'}],\n",
    "#  'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'},\n",
    "#  'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None},\n",
    "#           {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
    "\n",
    "feed = feedparser.parse(response.content)\n",
    "if feed.entries:\n",
    "    entry = feed.entries[0]\n",
    "    arxiv_id = entry.get('id', '')\n",
    "\n",
    "    # Ensure that the title matches\n",
    "    arxiv_title = re.sub(r\"\\s+\", \" \", entry.get('title', '')).strip()\n",
    "    authors = ', '.join([a.name for a in entry.get('authors', [])])\n",
    "\n",
    "    print(authors)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Gim,Guojun Chen,Seung-seob Lee,Nikhil Sarda,Anurag Khandelwal,Lin Zhong\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c60418cb20bd8b9e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
